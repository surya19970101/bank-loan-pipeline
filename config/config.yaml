# GCP projecregion
project_id: bank-loan-analytics        # e.g., bank-loan-analytics-12345
region: asia-south1                        # keep same region across services

# Google Cloud Storage settings
gcs:
  bucket: bank-loan-data-suraj             # the bucket you already created
  raw_prefix: data/raw/                    # folder/prefix for raw uploads
  processed_prefix: data/processed/        # folder/prefix for cleaned files

# BigQuery settings
bigquery:
  dataset: bank_loan                       # create this dataset in BQ
  staging_table: staging_loans             # raw load table
  curated_table: loans_curated             # transformed table
  partition_field: application_date        # choose an existing DATE/TIMESTAMP column
  clustering_fields: [loan_status, state]  # optional but useful for performance

# Airflow / Composer (optional for now; weâ€™ll use it later)
composer:
  dag_name: bank_loan_daily_etl

# Paths inside this repo (helps scripts & DAGs find assets)
paths:
  schema_file: sql/bq_schema_loans.json    # optional JSON schema file (we'll add later)
  transform_sql: sql/transform_loans.sql   # your main transformation SQL
